{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_sen_len = 100\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "EMBEDDING_FILE = 'embeddings/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for dataset\n",
    "    Every dataset is lower cased except\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\\\\", \"\", string)    \n",
    "    string = re.sub(r\"\\'\", \"\", string)    \n",
    "    string = re.sub(r\"\\\"\", \"\", string)    \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('input/imdb/labeledTrainData.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts, labels = [], []\n",
    "for i in range(train_df['review'].shape[0]):\n",
    "    text = train_df['review'][i]\n",
    "    texts.append(clean_str(text))\n",
    "    labels.append(train_df['sentiment'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tok.fit_on_texts(texts)\n",
    "seq = tok.texts_to_sequences(texts)\n",
    "data = pad_sequences(seq,maxlen=max_sen_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.expand_dims(data, axis=1)\n",
    "# data.shape\n",
    "# max_doc = data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = tok.word_index\n",
    "labels_cat = np.expand_dims(labels,axis=1)\n",
    "# labels_cat = to_categorical(np.asarray(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val = train_test_split(data,labels_cat,test_size=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(idx) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in idx.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/ashish/New Volume/Documents/Compressed/ML/Codes/Courses/Hierarchical_Text_Classification/model.py:156: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "model = createHierarchicalAttentionModel(max_sen_len, embWeights=embedding_matrix,\n",
    "                                                        embeddingSize = 300, vocabSize = min(MAX_NUM_WORDS, len(idx) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=5)\n",
    "callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22000 samples, validate on 3000 samples\n",
      "Epoch 1/4\n",
      "22000/22000 [==============================] - 103s 5ms/step - loss: 0.1440 - acc: 0.9517 - val_loss: 0.1516 - val_acc: 0.9477\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.94767, saving model to weights_base.best.hdf5\n",
      "Epoch 2/4\n",
      "22000/22000 [==============================] - 105s 5ms/step - loss: 0.1069 - acc: 0.9632 - val_loss: 0.1667 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.94767 to 0.94867, saving model to weights_base.best.hdf5\n",
      "Epoch 3/4\n",
      "22000/22000 [==============================] - 104s 5ms/step - loss: 0.0781 - acc: 0.9747 - val_loss: 0.1989 - val_acc: 0.9353\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/4\n",
      "22000/22000 [==============================] - 103s 5ms/step - loss: 0.0627 - acc: 0.9796 - val_loss: 0.2265 - val_acc: 0.9227\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6d7401a128>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=64, epochs=4,validation_data=(x_val, y_val),callbacks = callbacks_list,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('score ',score)\n",
    "# print('score ',loss)\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('input/imdb/testData.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_test = [] \n",
    "for i in range(test_df['review'].shape[0]):\n",
    "    text = test_df['review'][i]\n",
    "    texts_test.append(clean_str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_seq = tok.texts_to_sequences(texts_test)\n",
    "test_data = pad_sequences(test_seq,maxlen=max_sen_len)\n",
    "test_data = np.expand_dims(test_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred  [[9.9904817e-01]\n",
      " [3.6834122e-03]\n",
      " [4.4308049e-03]\n",
      " ...\n",
      " [4.3118285e-04]\n",
      " [9.9770278e-01]\n",
      " [4.5535466e-01]]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test_data,batch_size=1024)\n",
    "print('pred ',pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pred = []\n",
    "for i in pred:    \n",
    "    if i>0.9:\n",
    "        final_pred.append(1)\n",
    "    else: final_pred.append(0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = pd.DataFrame(data=[],columns=['id','sentiment'])#read_csv('sampleSubmission.csv')\n",
    "subm['sentiment'] = final_pred\n",
    "subm['id'] = test_df['id']\n",
    "subm.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
